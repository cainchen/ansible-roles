---
- name: "Check the target host if was already become a peer node within gluster"
  any_errors_fatal: true
  poll: 0
  shell: gluster peer status | grep -i uuid
  register: peer_check
  failed_when: peer_check.rc == 0

- name: "Disable firewalld"
  any_errors_fatal: true
  poll: 0
  shell: "{{ item }}"
  with_items:
    - systemctl stop firewalld
    - systemctl disable firewalld
  register: firewalld_disable
  failed_when: firewalld_disable.rc != 0

- name: "Check firewalld has disabled"
  any_errors_fatal: true
  poll: 0
  shell: "{{ item }}"
  with_items:
    - systemctl status firewalld
    - systemctl list-unit-files | grep firewalld | grep enabled
  register: firewalld_check
  failed_when: firewalld_check.rc == 0

- name: "Enable IPv6"
  any_errors_fatal: true
  block:
    - shell: sed -i 's/ipv6=1/ipv6=0/g' /etc/sysctl.conf
      register: enable_ipv6
      failed_when: enable_ipv6.rc != 0
    - shell: sysctl -p
      register: check_ipv6
      failed_when: check_ipv6.rc != 0

- name: "Deploy fdisk.sh"
  any_errors_fatal: true
  poll: 0
  template: "{{ item }}"
  with_items:
    - { src: "../templates/fdisk.sh.j2" ,  dest: "/tmp/fdisk.sh" }
  register: fdisk_sh
  failed_when: fdisk_sh.state != 'file'

- name: "Execute fdisk"
  any_errors_fatal: true
  poll: 0
  shell: sh /tmp/fdisk.sh
  register: fdisk_status
  failed_when: fdisk_status.rc != 0

- name: "Install gluster"
  any_errors_fatal: true
  poll: 0
  shell: "yum install -y {{ item }}"
  with_items:
    - centos-release-gluster41.noarch
    - glusterfs
    - glusterfs-fuse
    - glusterfs-cli
    - glusterfs-libs
    - glusterfs-server
  register: gluster_install
  failed_when: gluster_install.rc != 0

- name: "Start glusterd and enabled when boot"
  any_errors_fatal: true
  poll: 0
  shell: "{{ item }}"
  with_items:
    - systemctl start glusterd && sleep 1
    - systemctl start glusterfsd && sleep 1
    - systemctl enable glusterd
    - systemctl enable glusterfsd
  register: glusterd_enable
  failed_when: glusterd_enable.rc != 0

- name: "Check glusterd is in running and enabled when boot"
  any_errors_fatal: true
  poll: 0
  shell: "{{ item }}"
  with_items:
    - systemctl status glusterd
    - systemctl status glusterfsd
    - systemctl list-unit-files | grep glusterd | grep enabled
    - systemctl list-unit-files | grep glusterfsd | grep enabled
  register: glusterd_check
  failed_when: glusterd_check.rc != 0

- name: "Check if the gluster volumes id has existed"
  any_errors_fatal: true
  poll: 0
  shell: gluster volume info | grep Bricks
  register: volume_check
  ignore_errors: true
  failed_when: volume_check.rc == 0

- name: "Add the gluster peer nodes"
  any_errors_fatal: true
  poll: 0
  shell: gluster peer probe {{ item }}
  with_items:
    - "{{ play_hosts }}"
  delegate_to: "{{ ansible_play_hosts[0] }}"
  run_once: true
  register: probe_add
  failed_when: probe_add.rc != 0
  ignore_errors: true

- pause:
    seconds: 3

- name: "Create the folders for the gluster shared volumes"
  any_errors_fatal: true
  poll: 0
  shell: mkdir -p {{ item }}
  with_items:
    - "{{ shared_volume }}"
  register: sv_add
  failed_when: sv_add.rc != 0

- pause:
    seconds: 1

- name: "Create the gluster replica volumes"
  any_errors_fatal: true
  poll: 0
  shell: gluster volume create {{ item.0 }} replica {{ replica_num }} {{ ansible_play_hosts[0] }}:{{ item.1 }} \
         {{ ansible_play_hosts[1] }}:{{ item.1 }} force
  with_together:
    - "{{ gv_id }}"
    - "{{ shared_volume }}"
  delegate_to: "{{ ansible_play_hosts[0] }}"
  run_once: true
  register: rv_add
  failed_when: rv_add.rc != 0

- pause:
    seconds: 3

- name: "Enable the gluster shared volumes"
  any_errors_fatal: true
  poll: 0
  shell: |
         gluster volume set {{ item }} performance.cache-size {{ cache_size }}
         gluster volume set {{ item }} performance.flush-behind on
         gluster volume set {{ item }} performance.io-thread-count {{ thread_count }}
         gluster volume set {{ item }} performance.write-behind on
         sleep 1
         gluster volume start {{ item }}
  with_items:
    - "{{ gv_id }}"
  delegate_to: "{{ ansible_play_hosts[0] }}"
  run_once: true
  register: sv_enable
  failed_when: sv_enable.rc != 0

- pause:
    seconds: 3

- name: "Create the folders for mount the gluster shared volumes"
  any_errors_fatal: true
  poll: 0
  shell: mkdir -p {{ item }}
  with_items:
    - "{{ mount_target }}"
  register: folder_add
  failed_when: folder_add.rc != 0

- name: "Mount the gluster shared volumes"
  any_errors_fatal: true
  poll: 0
  shell: mount -t glusterfs {{ ansible_nodename }}:{{ item.0 }} {{ item.1 }} && sleep 1
  with_together:
    - "{{ gv_id }}"
    - "{{ mount_target }}"
  register: sv_mount
  failed_when: sv_mount.rc != 0

- pause:
    seconds: 2

- name: "Check the shared volumes has mounted on all nodes"
  any_errors_fatal: true
  poll: 0
  shell: df -h | grep -w {{ item }}
  with_items:
    - "{{ mount_target }}"
  register: sv_checkall
  failed_when: sv_checkall.rc != 0

- name: "Make sure the shared volumes be automatically mounted when the boot"
  lineinfile:
    dest: /etc/rc.d/rc.local
    state: present
    regexp: "{{ item.1 }}"
    line: mount -t glusterfs {{ ansible_nodename }}:{{ item.0 }} {{ item.1 }}
    mode: 0755
  with_together:
    - "{{ gv_id }}"
    - "{{ mount_target }}"

- pause:
    seconds: 2

- name: "Reboot node2"
  poll: 0
  async: 1
  shell: 'sleep 1 && shutdown -r now "Reboot triggered by Ansible"'
  when: node == 2

- pause:
    seconds: 4

- name: "Check gluster shared volumes has mounted on node 1"
  any_errors_fatal: true
  poll: 0
  shell: df -h | grep -w {{ item }}
  with_items:
    - "{{ mount_target }}"
  register: node1_sv_check
  failed_when: node1_sv_check.rc != 0
  when: node == 1

- name: "Wait for node 2 reboot"
  wait_for:
    host: "{{ ansible_play_hosts[1] }}"
    port: 22
    delay: 10
  connection: local
  when: node == 2

- name: "Reboot node 1"
  poll: 0
  async: 1
  shell: 'sleep 1 && shutdown -r now "Reboot triggered by Ansible"'
  when: node == 1

- pause:
    seconds: 4

- name: "Ensure gluster shared volumes has mounted on node 2"
  any_errors_fatal: true
  block:
   - shell: "mount -t glusterfs {{ ansible_nodename }}:{{ item.0 }} {{ item.1 }} && sleep 1"
     with_together:
       - "{{ gv_id }}"
       - "{{ mount_target }}"
     ignore_errors: true
     when: node == 2
   - shell: df -h | grep -w {{ item }}
     with_items:
       - "{{ mount_target }}"
     register: node2_sv_check
     failed_when: node2_sv_check.rc != 0
     when: node == 2

- name: "Wait for node 1 reboot"
  wait_for:
    host: "{{ ansible_play_hosts[0] }}"
    port: 22
    delay: 10
  connection: local
  when: node == 1

- pause:
    seconds: 2

- name: "Create testing files in the gluster shared volume on node 1"
  any_errors_fatal: true
  poll: 0
  shell: touch {{ item }}/gluster{1..10}
  with_items:
    - "{{ mount_target }}"
  register: files_create
  failed_when: files_create.rc != 0
  when: node == 1

- pause:
    seconds: 5

- name: "Check the node 2 has including files created on node 1"
  any_errors_fatal: true
  poll: 0
  shell: ls {{ item }}/gluster* | wc -l
  with_items:
    - "{{ mount_target }}"
  register: files_check
  failed_when: node == 2 and files_check.stdout != '10'

- name: "Remove testing materials"
  any_errors_fatal: true
  poll: 0
  shell: |
         rm -f /tmp/fdisk.sh
         rm -f /tmp/disk
         rm -f {{ item }}/gluster*
  with_items:
    - "{{ mount_target }}"
  register: remove
  failed_when: remove.rc != 0

- name: "Freed target's memory usage"
  poll: 0
  shell: sync ; echo 3 > /proc/sys/vm/drop_caches
